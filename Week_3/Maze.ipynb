{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61024f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 320/1000, success (last‑20): 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Maze Navigation with SNN + STDP (Enhanced)\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 1) Maze layout\n",
    "maze_grid = np.zeros((10, 10), dtype=int)\n",
    "maze_grid[1:9, 5] = 1\n",
    "maze_grid[5, 1:9] = 1\n",
    "\n",
    "# 2) Cardinal motions (N, E, S, W)\n",
    "motions = [\n",
    "    np.array([-1,  0]),  # North\n",
    "    np.array([ 0,  1]),  # East\n",
    "    np.array([ 1,  0]),  # South\n",
    "    np.array([ 0, -1]),  # West\n",
    "]\n",
    "\n",
    "# 3) Custom gym‐style env (uses grid directly)\n",
    "class CustomMazeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = gym.spaces.Box(0, 3, shape=maze_grid.shape, dtype=np.uint8)\n",
    "        self.action_space      = gym.spaces.Discrete(len(motions))\n",
    "        self.agent_pos = np.array([0,0])\n",
    "        self.goal      = np.array([9,9])\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = np.array([0,0])\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_dist = np.linalg.norm(self.agent_pos - self.goal)\n",
    "        # move if free\n",
    "        new = self.agent_pos + motions[action]\n",
    "        if (0 <= new[0] < 10 and 0 <= new[1] < 10 and maze_grid[new[0],new[1]]==0):\n",
    "            self.agent_pos = new\n",
    "        done = np.array_equal(self.agent_pos, self.goal)\n",
    "        # base reward at goal\n",
    "        reward = 1.0 if done else -0.01\n",
    "        # shaping: reduction in distance\n",
    "        new_dist = np.linalg.norm(self.agent_pos - self.goal)\n",
    "        reward += 0.1 * (old_dist - new_dist)\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = maze_grid.copy()\n",
    "        obs[tuple(self.agent_pos)] = 2\n",
    "        obs[tuple(self.goal)]      = 3\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        print(self._get_obs())\n",
    "\n",
    "env = CustomMazeEnv()\n",
    "\n",
    "# 4) SNN + STDP definitions\n",
    "class LIFNeuron:\n",
    "    def __init__(self, tau=20., v_thresh=1., v_reset=0.):\n",
    "        self.tau = tau; self.v_thresh = v_thresh; self.v_reset = v_reset; self.v = 0.\n",
    "    def step(self, I, dt=1.):\n",
    "        self.v += dt * (-self.v/self.tau + I)\n",
    "        if self.v >= self.v_thresh:\n",
    "            self.v = self.v_reset\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "class STDP_Synapse:\n",
    "    def __init__(self, w_init=0.5, A_plus=0.1, A_minus=0.12, tau_plus=20., tau_minus=20., w_min=0., w_max=1.):\n",
    "        self.w = w_init\n",
    "        self.A_plus, self.A_minus = A_plus, A_minus\n",
    "        self.tau_plus, self.tau_minus = tau_plus, tau_minus\n",
    "        self.w_min, self.w_max = w_min, w_max\n",
    "        self.pre_trace = 0.\n",
    "        self.post_trace = 0.\n",
    "    def update(self, pre_spike, post_spike):\n",
    "        self.pre_trace  *= np.exp(-1/self.tau_plus)\n",
    "        self.post_trace *= np.exp(-1/self.tau_minus)\n",
    "        if pre_spike:  self.pre_trace += 1.\n",
    "        if post_spike: self.post_trace += 1.\n",
    "        dw = self.A_plus * self.pre_trace * post_spike \\\n",
    "           - self.A_minus * self.post_trace * pre_spike\n",
    "        self.w = np.clip(self.w + dw, self.w_min, self.w_max)\n",
    "    def modulate_reward(self, reward):\n",
    "        # boost learning after reward\n",
    "        self.A_plus  *= (1 + reward)\n",
    "        self.A_minus *= (1 - reward)\n",
    "\n",
    "# 5) Build network: 4 wall sensors + 2 goal‐dir sensors → 4 actions\n",
    "def build_network(n_sensors=6, n_motors=4):\n",
    "    sensors  = [LIFNeuron(tau=15.) for _ in range(n_sensors)]\n",
    "    motors   = [LIFNeuron(tau=15.) for _ in range(n_motors)]\n",
    "    synapses = [[STDP_Synapse() for _ in range(n_motors)] for _ in range(n_sensors)]\n",
    "    return sensors, motors, synapses\n",
    "\n",
    "sensors, motors, synapses = build_network()\n",
    "\n",
    "# 6) Sensor function\n",
    "def get_sensor_signals(env):\n",
    "    pos = env.agent_pos\n",
    "    # 4 local wall detectors\n",
    "    sig = []\n",
    "    for m in motions:\n",
    "        new = pos + m\n",
    "        free = (0 <= new[0] < 10 and 0 <= new[1] < 10 and maze_grid[new[0],new[1]]==0)\n",
    "        sig.append(float(free))\n",
    "    # 2 goal‐direction signals\n",
    "    delta = env.goal - pos\n",
    "    dist  = np.linalg.norm(delta) + 1e-6\n",
    "    sig.append(delta[0]/dist)\n",
    "    sig.append(delta[1]/dist)\n",
    "    return sig\n",
    "\n",
    "# 7) Supervised R‑STDP Training Loop\n",
    "\n",
    "# --- Teacher: greedy action minimizing distance to goal ---\n",
    "def teacher_action(env):\n",
    "    pos, goal = env.agent_pos, env.goal\n",
    "    dists = []\n",
    "    for m in motions:\n",
    "        new = pos + m\n",
    "        if 0 <= new[0] < 10 and 0 <= new[1] < 10 and maze_grid[new[0],new[1]] == 0:\n",
    "            dists.append(np.linalg.norm(new - goal))\n",
    "        else:\n",
    "            dists.append(1e6)\n",
    "    return int(np.argmin(dists))\n",
    "\n",
    "# 7) Training loop with resets & input scaling\n",
    "\n",
    "history = []\n",
    "n_episodes = 1000\n",
    "max_steps  = 200\n",
    "\n",
    "# Input gains\n",
    "SENS_GAIN  = 3.0\n",
    "MOTOR_GAIN = 2.0\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    success = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # ---- 0) Reset all membrane potentials ----\n",
    "        for n in sensors + motors:\n",
    "            n.v = 0.0\n",
    "\n",
    "        # 1) Sensor → input currents (scaled) → spikes\n",
    "        I_sens   = get_sensor_signals(env)\n",
    "        spikes_s = [n.step(I=SENS_GAIN * i) for n, i in zip(sensors, I_sens)]\n",
    "\n",
    "        # 2) Motor drive: weighted sum → scaled → spikes\n",
    "        I_motors = []\n",
    "        for j in range(len(motors)):\n",
    "            Ij = sum(synapses[i][j].w * spikes_s[i] for i in range(len(sensors)))\n",
    "            I_motors.append(MOTOR_GAIN * Ij)\n",
    "\n",
    "        spikes_m = [n.step(I=Ij) for n, Ij in zip(motors, I_motors)]\n",
    "\n",
    "        # 3) Choose action\n",
    "        if sum(spikes_m) == 0:\n",
    "            action = np.random.randint(len(motors))\n",
    "        else:\n",
    "            action = int(np.argmax(spikes_m))\n",
    "\n",
    "        # 4) Teacher’s action\n",
    "        teach = teacher_action(env)\n",
    "\n",
    "        # 5) Execute\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # 6) STDP + supervised STDP + reward\n",
    "        for i in range(len(sensors)):\n",
    "            for j in range(len(motors)):\n",
    "                syn = synapses[i][j]\n",
    "                # (a) unsupervised STDP\n",
    "                syn.update(pre_spike=spikes_s[i], post_spike=spikes_m[j])\n",
    "                # (b) supervised “teacher” STDP\n",
    "                syn.update(pre_spike=spikes_s[i], post_spike=(1 if j == teach else 0))\n",
    "                # (c) reward‐modulated STDP on success\n",
    "                if reward > 0:\n",
    "                    syn.modulate_reward(reward)\n",
    "\n",
    "        if done:\n",
    "            success = 1\n",
    "            break\n",
    "\n",
    "    history.append(success)\n",
    "    if (ep + 1) % 20 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode {ep+1}/{n_episodes}, success (last‑20): {np.mean(history[-20:])*100:.1f}%\")\n",
    "\n",
    "# 8) Plot\n",
    "plt.plot(np.convolve(history, np.ones(10)/10, mode='valid'))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Success rate (10‑episode avg)')\n",
    "plt.title('SNN + R‑STDP (with resets & gains)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035d212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
