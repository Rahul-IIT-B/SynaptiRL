{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b54290",
   "metadata": {},
   "source": [
    "# Objective 1: Basic SNN for Binary Classification\n",
    "\n",
    "Using SNNTorch to distinguish two MNIST digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9236d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11f85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hyperparameters\n",
    "batch_size = 128\n",
    "num_steps   = 25\n",
    "beta        = 0.95\n",
    "lr          = 5e-4\n",
    "epochs      = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b2744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare binary MNIST (digits 0 and 1)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "full_train = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "full_test  = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "\n",
    "def filter_digits(dataset, digits=[0,1]):\n",
    "    idx = [i for i,(img,t) in enumerate(dataset) if t in digits]\n",
    "    return Subset(dataset, idx)\n",
    "\n",
    "train_data = filter_digits(full_train)\n",
    "test_data  = filter_digits(full_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40135bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Network (architecture unchanged)\n",
    "num_inputs  = 28*28\n",
    "num_hidden  = 1000\n",
    "num_outputs = 2  # binary\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2  = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.lif1.reset_mem()\n",
    "        self.lif2.reset_mem()\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec), torch.stack(mem2_rec)\n",
    "\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d76a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f600f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training and evaluation functions\n",
    "def train_epoch():\n",
    "    net.train()\n",
    "    total_loss = total_correct = total_samples = 0\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        spk_rec, mem_rec = net(data)\n",
    "\n",
    "        # compute loss summed over time\n",
    "        loss = sum(criterion(mem_rec[t], targets) for t in range(num_steps))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        # rate decoding via spike counts: sum over time\n",
    "        spikes = spk_rec.sum(dim=0)\n",
    "        preds = spikes.argmax(dim=1)\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "    return total_loss/total_samples, total_correct/total_samples\n",
    "\n",
    "\n",
    "def test_epoch():\n",
    "    net.eval()\n",
    "    total_loss = total_correct = total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            spk_rec, mem_rec = net(data)\n",
    "\n",
    "            loss = sum(criterion(mem_rec[t], targets) for t in range(num_steps))\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "\n",
    "            spikes = spk_rec.sum(dim=0)\n",
    "            preds = spikes.argmax(dim=1)\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "    return total_loss/total_samples, total_correct/total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a761059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train loss 2.437, acc 0.978 | Test loss 0.317, acc 1.000\n",
      "Epoch  2 | Train loss 0.356, acc 0.998 | Test loss 0.246, acc 0.997\n",
      "Epoch  3 | Train loss 0.273, acc 0.998 | Test loss 0.230, acc 0.998\n",
      "Epoch  4 | Train loss 0.261, acc 0.999 | Test loss 0.223, acc 1.000\n",
      "Epoch  5 | Train loss 0.183, acc 0.999 | Test loss 0.201, acc 0.999\n",
      "Epoch  6 | Train loss 0.132, acc 0.999 | Test loss 0.231, acc 0.999\n",
      "Epoch  7 | Train loss 0.171, acc 0.999 | Test loss 0.238, acc 0.999\n",
      "Epoch  8 | Train loss 0.091, acc 1.000 | Test loss 0.182, acc 0.999\n",
      "Epoch  9 | Train loss 0.121, acc 0.999 | Test loss 0.199, acc 0.997\n",
      "Epoch 10 | Train loss 0.112, acc 0.999 | Test loss 0.155, acc 0.998\n"
     ]
    }
   ],
   "source": [
    "# 7. Run training\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = train_epoch()\n",
    "    te_loss, te_acc = test_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss {tr_loss:.3f}, acc {tr_acc:.3f} | Test loss {te_loss:.3f}, acc {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5da1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Accuracy: 99.81%\n"
     ]
    }
   ],
   "source": [
    "# 8. Final test accuracy\n",
    "te_loss, te_acc = test_epoch()\n",
    "print(f\"\\nFinal Test Set Accuracy: {100*te_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
